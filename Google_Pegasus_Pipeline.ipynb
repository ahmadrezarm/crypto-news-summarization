{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium\n",
        "!apt-get update \n",
        "!apt install chromium-chromedriver\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.common.exceptions import TimeoutException\n",
        "from selenium.webdriver.support.wait import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "driver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n",
        "\n",
        "import pandas as pd\n",
        "! pip install transformers\n",
        "! pip install sentencepiece\n",
        "from transformers import pipeline\n",
        "summarizer = pipeline(\"summarization\", model=\"google/pegasus-xsum\", device = 0)\n",
        "sentiment_tagger = pipeline(\"sentiment-analysis\", model=\"ProsusAI/finbert\", device = 0)\n",
        "'''\n",
        "sentiment_translator = {\n",
        "  \"Bullish\": \"positive\",\n",
        "  \"Bearish\": \"negative\",\n",
        "  \"Neutral\": \"neutral\"\n",
        "}\n",
        "'''\n",
        "\n",
        "\n",
        "#human-centered-summarization/financial-summarization-pegasus\n",
        "# facebook/bart-large-cnn\n",
        "\n",
        "\n",
        "# add Firebase credentials here\n",
        "\n",
        "\n",
        "# the coins list needs to be imported \n",
        "coins_list = pd.read_csv('/content/coins.csv')"
      ],
      "metadata": {
        "id": "3yRyiO6MuGfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# news classification function:\n",
        "\n",
        "def categorize(title):\n",
        "  contained_coins = []\n",
        "  for index in coins_list.index:\n",
        "    if coins_list['name'][index] in title.split():\n",
        "      contained_coins.append(coins_list['name'][index])\n",
        "    if coins_list['symbol'][index] in title.split():\n",
        "      contained_coins.append(coins_list['symbol'][index])\n",
        "    if coins_list['slug'][index] in title.split():\n",
        "      contained_coins.append(coins_list['slug'][index])\n",
        "  return contained_coins\n",
        "\n",
        "\n",
        "# function for checking if the article is Price Prediction or not:\n",
        "def is_price_prediction(title):\n",
        "  if 'price prediction' in title.lower():\n",
        "    return \"yes\"\n",
        "  else:\n",
        "    return \"no\"\n",
        "\n",
        "\n",
        "# function for checking if the article is newsletter or not:\n",
        "def is_news_letter(title):\n",
        "  if 'newsletter' in title.lower():\n",
        "    return \"yes\"\n",
        "  else:\n",
        "    return \"no\""
      ],
      "metadata": {
        "id": "g3d5ybBAvUry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instead of para by para, give it all to the model:\n",
        "\n",
        "# getting articles from Cointelegraph, categorizing, and checking for price prediction and newsletter:\n",
        "driver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n",
        "\n",
        "url = 'https://cointelegraph.com/category/latest-news'\n",
        "driver.get(url)\n",
        "\n",
        "articles = driver.find_elements(By.CLASS_NAME, \"post-card-inline\")\n",
        "#####links_list = []\n",
        "for article in articles:\n",
        "  title_tag = article.find_element(By.XPATH, \".//*[@class = 'post-card-inline__title-link']\")\n",
        "  title = title_tag.text\n",
        "  if len(categorize(title)) != 0:\n",
        "    if (is_price_prediction(title) == 'no') & (is_news_letter(title) == 'no'):\n",
        "      link = title_tag.get_attribute('href')\n",
        "      print(title)\n",
        "      # push to firebase here: Push the tile, the link, and the source (Cointelegraph).\n",
        "      ######links_list.append(link)\n",
        "      driver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n",
        "      driver.get(link)\n",
        "      tag = WebDriverWait(driver, 25).until(\n",
        "      EC.presence_of_all_elements_located((By.XPATH, \".//div[@class='post-content']//p\")))\n",
        "      #print(len(tag))\n",
        "      i = 1\n",
        "      summary = ' '\n",
        "      full_text = ' '\n",
        "      while i < len(tag):\n",
        "        try:\n",
        "          text = WebDriverWait(driver, 50).until(\n",
        "          EC.presence_of_element_located((By.XPATH, f\".//div[@class='post-content']//p[{i}]\"))).text\n",
        "          if (len(text.split()) >= 5) & ('Related: ' not in text):\n",
        "            print('paragraph: ',text)\n",
        "            print('lengh of the paragraph is: ', len(text.split()))\n",
        "            full_text = full_text+ text + \". \"\n",
        "\n",
        "            # max_length= round(0.8*len(text.split())), min_length= round(0.2*len(text.split())), \n",
        "          i+=1\n",
        "        except TimeoutException:\n",
        "          #print(\"Done\")\n",
        "          break\n",
        "  try:\n",
        "    print('full_text is: ', full_text) \n",
        "    print('full_text lenthg is: ', len(full_text.split())) \n",
        "    summary = summarizer(full_text,max_length= round(0.7*len(full_text.split())), min_length= round(0.2*len(full_text.split())), do_sample=False)[0][\"summary_text\"] \n",
        "    print(\"lenght of summary is: \", len(summary.split()))\n",
        "    print('summary is: ', summary)\n",
        "    sentiment = sentiment_tagger(summary, max_length=512, truncation = True)[0]['label']\n",
        "    print('sentiment is: ', sentiment) \n",
        "    print(\"_\"*80) \n",
        "\n",
        "  except NameError:\n",
        "    continue\n"
      ],
      "metadata": {
        "id": "sa-vXEnuVmG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######### test\n",
        "\n",
        "\n",
        "\n",
        "# instead of para by para, give it all to the model:\n",
        "\n",
        "# getting articles from Cointelegraph, categorizing, and checking for price prediction and newsletter:\n",
        "driver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n",
        "\n",
        "url = 'https://cointelegraph.com/category/latest-news'\n",
        "driver.get(url)\n",
        "\n",
        "articles = driver.find_elements(By.CLASS_NAME, \"post-card-inline\")\n",
        "#####links_list = []\n",
        "for article in articles:\n",
        "  title_tag = article.find_element(By.XPATH, \".//*[@class = 'post-card-inline__title-link']\")\n",
        "  title = title_tag.text\n",
        "  print(title)\n",
        "  print(title_tag.get_attribute('href'))\n",
        "  print(len(categorize(title)))\n",
        "  if len(categorize(title)) != 0:\n",
        "    if (is_price_prediction(title) == 'no') & (is_news_letter(title) == 'no'):\n",
        "      link = title_tag.get_attribute('href')\n",
        "      # push to firebase here: Push the tile, the link, and the source (Cointelegraph).\n",
        "      ######links_list.append(link)\n",
        "      driver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n",
        "      driver.get(link)\n",
        "      tag = WebDriverWait(driver, 25).until(\n",
        "      EC.presence_of_all_elements_located((By.XPATH, \".//div[@class='post-content']//p\")))\n",
        "      #print(len(tag))\n",
        "      i = 1\n",
        "      summary = ' '\n",
        "      full_text = ' '\n",
        "      while i < len(tag):\n",
        "        try:\n",
        "          text = WebDriverWait(driver, 50).until(\n",
        "          EC.presence_of_element_located((By.XPATH, f\".//div[@class='post-content']//p[{i}]\"))).text\n",
        "          if (len(text.split()) >= 5) & ('Related: ' not in text):\n",
        "            print('paragraph: ',text)\n",
        "            print('lengh of the paragraph is: ', len(text.split()))\n",
        "            full_text = full_text+ text + \". \"\n",
        "\n",
        "            # max_length= round(0.8*len(text.split())), min_length= round(0.2*len(text.split())), \n",
        "          i+=1\n",
        "        except TimeoutException:\n",
        "          #print(\"Done\")\n",
        "          break\n",
        "  try:\n",
        "    print('full_text is: ', full_text) \n",
        "    print('full_text lenthg is: ', len(full_text.split())) \n",
        "    summary = summarizer(full_text,max_length= round(0.7*len(full_text.split())), min_length= round(0.2*len(full_text.split())), do_sample=False)[0][\"summary_text\"] \n",
        "    print(\"lenght of summary is: \", len(summary.split()))\n",
        "    print('summary is: ', summary)\n",
        "    sentiment = sentiment_tagger(summary, max_length=512, truncation = True)[0]['label']\n",
        "    print('sentiment is: ', sentiment) \n",
        "    print(\"_\"*80) \n",
        "\n",
        "  except NameError:\n",
        "    continue\n"
      ],
      "metadata": {
        "id": "tFpwhcHEmAXz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}